{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cloudscraper as cs\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from google.cloud import language_v1\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"raw-data-392614-6e7d6f0fa9e1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4acd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(os.getcwd())\n",
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords') #download stopword corpus\n",
    "stopwords.words('english')[0:10] # Show some stop words tp verify install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    def __init__(self, domain):\n",
    "        self.domain = domain\n",
    "        \n",
    "    def grab_details(self):\n",
    "        scraper = cs.create_scraper()\n",
    "        data = scraper.get(self.domain)\n",
    "        return data\n",
    "    \n",
    "    def text_process(self, content):\n",
    "        \"\"\"\n",
    "        Takes in a string of text, then performs the following:\n",
    "        1. Remove all punctuation\n",
    "        2. Remove all stopwords\n",
    "        3. Returns a list of the cleaned text\n",
    "        \"\"\"\n",
    "        # Check characters to see if they are in punctuation\n",
    "        nopunc = [char for char in content if char not in string.punctuation]\n",
    "\n",
    "        # Join the characters again to form the string.\n",
    "        nopunc = ''.join(nopunc)\n",
    "\n",
    "        # Now just remove any stopwords\n",
    "        return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "        \n",
    "    def grab_content(self):\n",
    "        try:\n",
    "            data = self.grab_details()\n",
    "            h1_all:str = \"\"\n",
    "            p_all:str = \"\"\n",
    "            h2_all:str = \"\"\n",
    "            h3_all:str = \"\"\n",
    "            allthecontent:str = \"\"\n",
    "\n",
    "            if self.grab_details().status_code != 200:\n",
    "                return None\n",
    "\n",
    "            soup = BeautifulSoup(data.text, 'html.parser')\n",
    "            title = soup.find('title').text\n",
    "            description = soup.find('meta', attrs={'name': 'description'})\n",
    "\n",
    "            if \"content\" in str(description):\n",
    "                description = description.get(\"content\")\n",
    "            else:\n",
    "                description = \"\"\n",
    "\n",
    "\n",
    "            h1 = soup.find_all('h1')\n",
    "            for x in range (len(h1)):\n",
    "                h1_all = h1_all + \" \" + h1[x].text\n",
    "                \n",
    "            h2 = soup.find_all('h2')\n",
    "            for x in range (len(h2)):\n",
    "                h2_all = h2_all + \" \" + h2[x].text\n",
    "\n",
    "            h3 = soup.find_all('h3')\n",
    "            for x in range (len(h3)):\n",
    "                h3_all = h3_all + \" \" + h3[x].text\n",
    "\n",
    "            paragraphs = soup.find_all('p')\n",
    "            for x in range (len(paragraphs)):\n",
    "                p_all = p_all + \" \" + paragraphs[x].text\n",
    "\n",
    "            allthecontent = title + \" \" + description + \" \" + h1_all + \" \" + h2_all + \" \" + h3_all + \" \" + p_all + \" \"\n",
    "            allthecontent = self.text_process(allthecontent)\n",
    "            allthecontent = allthecontent[0:999]\n",
    "            allthecontent = \" \".join(allthecontent)\n",
    "            return allthecontent\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf61af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(urlList:list):\n",
    "    result = pd.DataFrame(columns=['URL', \"Text\", \"Category\", \"Confidence\", \"Length\"])\n",
    "    language_client = language_v1.LanguageServiceClient()\n",
    "    \n",
    "    for url in urlList:\n",
    "        website = Website(url)\n",
    "        content = website.grab_content()\n",
    "\n",
    "        document = language_v1.Document(\n",
    "            content=content, type_=language_v1.Document.Type.PLAIN_TEXT\n",
    "        )\n",
    "        response = language_client.classify_text(request={\"document\": document})\n",
    "        categories = response.categories\n",
    "\n",
    "        for category in categories:\n",
    "            result = result.append({\"URL\":url, \"Text\":content, \"Category\":category.name, \"Confidence\":category.confidence, \"Length\": len(content)}, ignore_index=True)\n",
    "        \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab24cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.disney.com\", \"https://wpengine.com\", \"https://deltiasgaming.com\", \"https://velocitize.com\"]\n",
    "\n",
    "classification = classify(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = classification.groupby(\"URL\",as_index=False).max()\n",
    "clean[clean[\"Confidence\"] > .8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124436ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification[\"Length\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca599c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
